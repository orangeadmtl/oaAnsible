#!/bin/bash
# oaSentinel Setup Script for Ubuntu ML Training
# Generated by Ansible for {{ inventory_hostname }}

set -e

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
CYAN='\033[0;36m'
NC='\033[0m'

# Configuration
OASENTINEL_HOME="{{ oasentinel_dir }}"
VENV_PATH="$OASENTINEL_HOME/.venv"
CONFIG_FILE="configs/ubuntu_gpu.yaml"

# Shared CrowdHuman dataset paths
CROWDHUMAN_ROOT="{{ ubuntu_crowdhuman_dataset.base_path }}"
CROWDHUMAN_RAW="{{ ubuntu_crowdhuman_dataset.raw_path }}"
CROWDHUMAN_PROCESSED="{{ ubuntu_crowdhuman_dataset.processed_path }}"
CROWDHUMAN_CONFIG="{{ ubuntu_crowdhuman_dataset.config_path }}"

# Logging functions
log_info() { echo -e "${BLUE}[INFO]${NC} $1"; }
log_success() { echo -e "${GREEN}[SUCCESS]${NC} $1"; }
log_warning() { echo -e "${YELLOW}[WARNING]${NC} $1"; }
log_error() { echo -e "${RED}[ERROR]${NC} $1"; }
log_header() { echo -e "\n${CYAN}===== $1 =====${NC}"; }

show_usage() {
    echo "oaSentinel Setup Script for Ubuntu"
    echo ""
    echo "Usage: $0 [options]"
    echo ""
    echo "Options:"
    echo "  --download-only    Download datasets without processing"
    echo "  --process-only     Process datasets without downloading"  
    echo "  --no-data         Skip data preparation entirely"
    echo "  --verify-only     Only verify environment setup"
    echo "  --force-reinstall  Force reinstall Python dependencies"
    echo "  -h, --help        Show this help message"
    echo ""
}

# Parse arguments
DOWNLOAD_DATA=true
PROCESS_DATA=true
VERIFY_ONLY=false
FORCE_REINSTALL=false

while [[ $# -gt 0 ]]; do
    case $1 in
        --download-only)
            PROCESS_DATA=false
            shift
            ;;
        --process-only)
            DOWNLOAD_DATA=false
            shift
            ;;
        --no-data)
            DOWNLOAD_DATA=false
            PROCESS_DATA=false
            shift
            ;;
        --verify-only)
            VERIFY_ONLY=true
            DOWNLOAD_DATA=false
            PROCESS_DATA=false
            shift
            ;;
        --force-reinstall)
            FORCE_REINSTALL=true
            shift
            ;;
        -h|--help)
            show_usage
            exit 0
            ;;
        *)
            log_error "Unknown option: $1"
            show_usage
            exit 1
            ;;
    esac
done

# Navigate to project directory
cd "$OASENTINEL_HOME"

log_header "oaSentinel Environment Setup"
log_info "Project directory: $OASENTINEL_HOME"
log_info "Configuration: $CONFIG_FILE"
log_info "Host: {{ inventory_hostname }}"

# Verify virtual environment
if [ ! -f "$VENV_PATH/bin/activate" ]; then
    log_error "Virtual environment not found at $VENV_PATH"
    log_info "Run Ansible oasentinel-setup tag to initialize"
    exit 1
fi

# Activate virtual environment
log_info "Activating virtual environment..."
source "$VENV_PATH/bin/activate"

# Verify Python packages
if [ "$FORCE_REINSTALL" = true ]; then
    log_info "Force reinstalling Python dependencies..."
    pip install --upgrade pip uv
    if [ -f "pyproject.toml" ]; then
        uv pip install --force-reinstall -e .
    else
        uv pip install --force-reinstall torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128
        uv pip install --force-reinstall ultralytics opencv-python wandb huggingface_hub datasets pyyaml typer rich
    fi
fi

# System verification
log_header "System Verification"

# Check GPU availability
if command -v nvidia-smi >/dev/null 2>&1; then
    log_success "NVIDIA GPU detected:"
    nvidia-smi --query-gpu=name,memory.total,driver_version --format=csv,noheader
    
    # Check CUDA in Python
    python3 -c "
import torch
print(f'PyTorch version: {torch.__version__}')
print(f'CUDA available: {torch.cuda.is_available()}')
if torch.cuda.is_available():
    print(f'CUDA version: {torch.version.cuda}')
    print(f'GPU count: {torch.cuda.device_count()}')
    for i in range(torch.cuda.device_count()):
        print(f'  GPU {i}: {torch.cuda.get_device_name(i)}')
    print('[OK] GPU acceleration ready!')
else:
    print('[WARNING] CUDA not available in PyTorch')
    " 2>/dev/null || log_warning "Could not verify PyTorch CUDA installation"
else
    log_warning "No NVIDIA GPU detected - training will use CPU"
fi

# Check core dependencies
log_info "Verifying core dependencies..."
python3 -c "
import sys
required_packages = ['torch', 'ultralytics', 'cv2', 'yaml', 'datasets']
missing = []

for pkg in required_packages:
    try:
        __import__(pkg)
        print(f'[OK] {pkg}')
    except ImportError:
        print(f'[FAIL] {pkg}')
        missing.append(pkg)

if missing:
    print(f'Missing packages: {missing}')
    sys.exit(1)
else:
    print('[OK] All core dependencies available')
" || {
    log_error "Missing core dependencies"
    log_info "Run with --force-reinstall to fix"
    exit 1
}

if [ "$VERIFY_ONLY" = true ]; then
    log_success "Environment verification completed"
    exit 0
fi

# Create necessary directories
log_info "Creating project directories..."
mkdir -p data/{raw,processed,splits}
mkdir -p models/{checkpoints,exports,torch_cache}
mkdir -p logs/{training,evaluation}
mkdir -p outputs/{visualizations,predictions}
mkdir -p cache/{cuda,datasets}

# Check configuration file
if [ ! -f "$CONFIG_FILE" ]; then
    log_warning "Configuration file not found: $CONFIG_FILE"
    log_info "Using default configuration"
    CONFIG_FILE="configs/default.yaml"
fi

# Data preparation (using shared location)
if [ "$DOWNLOAD_DATA" = true ]; then
    log_header "Dataset Download (Shared Location)"
    log_info "Downloading to shared location: $CROWDHUMAN_RAW"
    
    if [ -f "bin/oas-download" ]; then
        chmod +x bin/oas-download
        python bin/oas-download --output "$CROWDHUMAN_RAW" || {
            log_warning "Dataset download failed or incomplete"
            log_info "You can download manually or run this script again"
        }
    else
        log_warning "bin/oas-download not found"
        log_info "Manual dataset setup may be required"
    fi
fi

if [ "$PROCESS_DATA" = true ]; then
    log_header "Dataset Processing (Shared Location)"
    log_info "Processing from: $CROWDHUMAN_RAW"
    log_info "Processing to: $CROWDHUMAN_PROCESSED"
    
    if [ -f "bin/oas-process" ]; then
        chmod +x bin/oas-process
        python bin/oas-process --input "$CROWDHUMAN_RAW" --output "$CROWDHUMAN_PROCESSED" || {
            log_warning "Dataset processing failed or incomplete"
            log_info "Check raw dataset directory and run oas-process manually"
        }
    else
        log_warning "bin/oas-process not found"
        log_info "Manual dataset processing may be required"
    fi
fi

# Verify dataset (using shared location)
log_header "Dataset Verification"

# Check shared dataset configuration
if [ -f "$CROWDHUMAN_CONFIG/dataset.yaml" ]; then
    log_success "Shared dataset configuration found: $CROWDHUMAN_CONFIG/dataset.yaml"
elif [ -f "crowdhuman.yaml" ]; then
    log_success "Project dataset configuration found: crowdhuman.yaml"
else
    log_warning "No dataset configuration found"
    log_info "Dataset may need to be prepared before training"
fi

# Verify shared dataset structure
log_info "Verifying shared dataset structure..."
python3 -c "
import os
from pathlib import Path

crowdhuman_processed = Path('$CROWDHUMAN_PROCESSED')
train_path = crowdhuman_processed / 'images' / 'train'
val_path = crowdhuman_processed / 'images' / 'val'

if train_path.exists():
    train_count = len([f for f in train_path.glob('*.jpg')])
    print(f'[OK] Training images: {train_count}')
else:
    print(f'[FAIL] Training path not found: {train_path}')

if val_path.exists():
    val_count = len([f for f in val_path.glob('*.jpg')])
    print(f'[OK] Validation images: {val_count}')
else:
    print(f'[FAIL] Validation path not found: {val_path}')

# Check raw dataset
raw_images = Path('$CROWDHUMAN_RAW') / 'Images'
if raw_images.exists():
    raw_count = len([f for f in raw_images.glob('*.jpg')])
    print(f'[OK] Raw images: {raw_count}')
else:
    print(f'[FAIL] Raw images not found: {raw_images}')
" 2>/dev/null || log_warning "Could not verify dataset structure"

# Setup complete
log_header "Setup Complete"
log_success "oaSentinel environment is ready for training!"
log_info ""
log_info "Quick Start Commands:"
log_info "  oas-train                    # Start training with default config"
log_info "  oas-train --config $CONFIG_FILE  # Use Ubuntu GPU config"
log_info "  oas-quick-train              # Start training in background"
log_info "  oas-screen-attach            # Attach to training session"
log_info ""
log_info "ðŸ“Š Monitoring Commands:"
log_info "  oas-logs                     # View training logs"
log_info "  gpu-watch                    # Monitor GPU usage"  
log_info "  oas-info                     # Show environment status"
log_info ""
log_info "ðŸ”§ Configuration:"
log_info "  Training config: $CONFIG_FILE"
log_info "  Environment file: .env"
log_info "  Virtual environment: $VENV_PATH"
log_info ""

# Show system summary
log_info "ðŸ“ˆ System Summary:"
log_info "  Host: {{ inventory_hostname }}"
log_info "  CPU: {{ ansible_processor_vcpus | default('N/A') }} cores"
log_info "  Memory: {{ ((ansible_memtotal_mb | default(8192))/1024)|round|int }}GB"
{% if ml_has_nvidia_gpu is defined and ml_has_nvidia_gpu %}
log_info "  GPU: NVIDIA (CUDA ready)"
{% else %}
log_info "  GPU: None (CPU training only)"
{% endif %}
log_info "  Storage: {{ ((ansible_mounts[0].size_total | default(1073741824000))/1024/1024/1024)|round|int }}GB total"
log_info ""
log_info "ðŸ“‚ Dataset Locations:"
log_info "  Shared CrowdHuman: $CROWDHUMAN_ROOT"
log_info "  Raw dataset: $CROWDHUMAN_RAW"
log_info "  Processed dataset: $CROWDHUMAN_PROCESSED"
log_info "  Configuration: $CROWDHUMAN_CONFIG"

log_info ""
log_success "Ready to train with shared dataset! ðŸŽ¯"